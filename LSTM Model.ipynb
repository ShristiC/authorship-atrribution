{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0   author  label  \\\n",
      "0               0   austen      0   \n",
      "1               1   austen      0   \n",
      "2               2   austen      0   \n",
      "3               3   austen      0   \n",
      "4               4   austen      0   \n",
      "...           ...      ...    ...   \n",
      "18888       18888  whitman     11   \n",
      "18889       18889  whitman     11   \n",
      "18890       18890  whitman     11   \n",
      "18891       18891  whitman     11   \n",
      "18892       18892  whitman     11   \n",
      "\n",
      "                                                    text  \n",
      "0      [Emma Jane Austen 1816] VOLUME I CHAPTER I Emm...  \n",
      "1      Even Miss Taylor ceased hold nominal office go...  \n",
      "2      It wedding-day beloved friend Emma sat mournfu...  \n",
      "3      The want Miss Taylor felt hour day.She recalle...  \n",
      "4      She dearly loved father, companion her.He meet...  \n",
      "...                                                  ...  \n",
      "18888  Mirages More experiences sights, stranger, you...  \n",
      "18889  The Unexpress'd How dare it?After cycles, poem...  \n",
      "18890  More evolutionary, vast, puzzling, O soul!More...  \n",
      "18891  Farewell dear mate, dear love!I'm going away, ...  \n",
      "18892  May-be mortal knob undoing, turning--so finall...  \n",
      "\n",
      "[18893 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def data_retrieval(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "df = data_retrieval('gutenberg_expanded.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# determining average word count per text\n",
    "word_count = []\n",
    "for i in df['text'].values:\n",
    "    word_count.append(len(i.split()))\n",
    "word_count = np.array(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(var):\n",
    "    \"\"\"Print summary statistics for a variable of interest.\n",
    "    \n",
    "    Args:\n",
    "    var: array. Numpy array containing values for the variable of interest.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Min:\", np.min(var))\n",
    "    print(\"Max:\", np.max(var))\n",
    "    print(\"Mean:\", np.mean(var))\n",
    "    print(\"Median\", np.median(var))\n",
    "    print(\"1st percentile\", np.percentile(var, 1))\n",
    "    print(\"95th percentile\", np.percentile(var, 95))\n",
    "    print(\"99th percentile\", np.percentile(var, 99))\n",
    "    print(\"99.5th Percentile\", np.percentile(var, 99.5))\n",
    "    print(\"99.9th Percentile\", np.percentile(var, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count statistics\n",
      "Min: 1\n",
      "Max: 875\n",
      "Mean: 55.006351558778384\n",
      "Median 49.0\n",
      "1st percentile 8.0\n",
      "95th percentile 115.0\n",
      "99th percentile 181.0799999999981\n",
      "99.5th Percentile 221.0\n",
      "99.9th Percentile 319.0800000000381\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count statistics\")\n",
    "get_stats(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot word count distribution\n",
    "sns.distplot(word_count, kde = False, bins = 70, color = 'blue').set_title(\"Word Count Distribution\")\n",
    "plt.xlabel('Excerpt Length (Words)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 500)\n",
    "plt.savefig(\"word_count.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15114,) (15114,)\n",
      "(1889,) (1889,)\n",
      "(1890,) (1890,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "text = df['text'].values\n",
    "labels = df['label'].values\n",
    "# splitting data into 80-20 for training and testing\n",
    "text_train, text_test_val, label_train, label_test_val = train_test_split(text,labels, test_size = 0.2, random_state = 42)\n",
    "# splitting testing data into 50-50 for testing and validation\n",
    "text_test, text_val, label_test, label_val = train_test_split(text_test_val, label_test_val, test_size=.5, random_state=42)\n",
    "print(text_train.shape, label_train.shape)\n",
    "print(text_test.shape, label_test.shape)\n",
    "print(text_val.shape, label_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shris\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# removing stop words\n",
    "def remove_stop_words_preprocessing(text_values):\n",
    "    for i in range(len(text_values)):\n",
    "        text_values[i] = remove_stopwords(text_values[i].lower())\n",
    "    return text_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# word embeddings\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def word_embeddings(text_values):\n",
    "    tokenizer = Tokenizer(num_words=5000, oov_token=\"UNK\")\n",
    "    tokenizer.fit_on_texts(text_values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41126 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Removing the stop words\n",
    "X = remove_stop_words_preprocessing(text_train)\n",
    "X_val_text = remove_stop_words_preprocessing(text_val)\n",
    "X_test_text = remove_stop_words_preprocessing(text_test)\n",
    "\n",
    "# Creating a tokenization\n",
    "tokenizer = word_embeddings(X)\n",
    "\n",
    "# Tokenizing the sentences\n",
    "X_train = tokenizer.texts_to_sequences(X)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41127\n"
     ]
    }
   ],
   "source": [
    "# vocab size\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (15114, 80)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 80\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=max_sequence_length)\n",
    "X_val = pad_sequences(X_val, padding=\"post\", maxlen=max_sequence_length)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=max_sequence_length)\n",
    "print('Shape of data tensor:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor:  (15114, 12)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(label_train).values\n",
    "Y_val = pd.get_dummies(label_val).values\n",
    "Y_test = pd.get_dummies(label_test).values\n",
    "print('Shape of label tensor: ', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 50)            2056350   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 80, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 2,117,962\n",
      "Trainable params: 2,117,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "callback = callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                   output_dim=embedding_dim,\n",
    "                  input_length=max_sequence_length))\n",
    "model.add(layers.SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(layers.Dense(12, activation='softmax'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shris\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15114 samples, validate on 1890 samples\n",
      "Epoch 1/15\n",
      "15114/15114 [==============================] - 92s 6ms/step - loss: 1.6924 - accuracy: 0.4153 - val_loss: 1.5483 - val_accuracy: 0.4899\n",
      "Epoch 2/15\n",
      "15114/15114 [==============================] - 90s 6ms/step - loss: 1.4918 - accuracy: 0.4711 - val_loss: 1.3106 - val_accuracy: 0.5476\n",
      "Epoch 3/15\n",
      "15114/15114 [==============================] - 95s 6ms/step - loss: 1.1831 - accuracy: 0.5742 - val_loss: 0.9552 - val_accuracy: 0.6624\n",
      "Epoch 4/15\n",
      "15114/15114 [==============================] - 95s 6ms/step - loss: 0.9466 - accuracy: 0.6555 - val_loss: 0.8074 - val_accuracy: 0.6899\n",
      "Epoch 5/15\n",
      "15114/15114 [==============================] - 95s 6ms/step - loss: 0.7570 - accuracy: 0.7244 - val_loss: 0.6859 - val_accuracy: 0.7487\n",
      "Epoch 6/15\n",
      "15114/15114 [==============================] - 103s 7ms/step - loss: 0.6432 - accuracy: 0.7752 - val_loss: 0.5958 - val_accuracy: 0.7931\n",
      "Epoch 7/15\n",
      "15114/15114 [==============================] - 97s 6ms/step - loss: 0.5556 - accuracy: 0.8188 - val_loss: 0.5047 - val_accuracy: 0.8217\n",
      "Epoch 8/15\n",
      "15114/15114 [==============================] - 97s 6ms/step - loss: 0.4228 - accuracy: 0.8544 - val_loss: 0.4315 - val_accuracy: 0.8534\n",
      "Epoch 9/15\n",
      "15114/15114 [==============================] - 98s 6ms/step - loss: 0.3306 - accuracy: 0.8871 - val_loss: 0.3642 - val_accuracy: 0.8804\n",
      "Epoch 10/15\n",
      "15114/15114 [==============================] - 98s 6ms/step - loss: 0.2507 - accuracy: 0.9154 - val_loss: 0.3040 - val_accuracy: 0.9026\n",
      "Epoch 11/15\n",
      " 4550/15114 [========>.....................] - ETA: 1:11 - loss: 0.2055 - accuracy: 0.9314"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=15, \n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    batch_size=25,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have shape (80,) but got array with shape (38855,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9b6befd3938e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (80,) but got array with shape (38855,)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "prediction = model.predict(X_test)\n",
    "confusion_matrix = confusion_matrix(Y_test.argmax(axis=1), prediction.argmax(axis=1))\n",
    "print(confusion_matrix)\n",
    "\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accuracy[0],accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Against Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test  = vectorizer.transform(text_test)\n",
    "X_val = vectorizer.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9603693943885853\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifier = LogisticRegression(multi_class='multinomial', solver=\"lbfgs\")\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(classifier, X_train, label_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# score = classifier.score(X_test, Y_test)\n",
    "print(\"Accuracy:\", mean(n_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial / Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram Distribution Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
